#initialize a df from lists;
d = {'col1': [1, 2], 'col2': [3, 4]}
df = pd.DataFrame(data=d, dtype=np.int8)

#empty df
df = pd.DataFrame(columns=['A','B','C','D','E','F','G'])

#build from dict
mydata = [{'subid' : 'B14-111', 'age': 75, 'fdg':1.78},
          {'subid' : 'B14-112', 'age': 22, 'fdg':1.56},]
df = pandas.DataFrame(mydata)


columns=['2002','2003','2004']
index=['Zidane','Figo','Beckham']
df = pd.DataFrame(columns=columns,index=index)
print df
        2002 2003 2004
Zidane  NaN   NaN NaN
Figo    NaN   NaN NaN
Beckham NaN   NaN NaN

#flatten list of list
text_ner_segmented = sum(text_ner_segmented, [])


# process pool
from multiprocessing import Pool
def worker_nlp_sent(txt):
  return something
txt_all = [txt1, txt2,]
pool = Pool(4)
text_ner_matrix = pool.map(worker_nlp_sent, txt_all)
pool.close()
pool.join()

#doc2vec model
from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Word2Vec
doc1 = TaggedDocument(words=text_tokens1, tags=['SENT_'+str(1)])
doc2 = TaggedDocument(words=text_tokens2, tags=['SENT_'+str(2)])
model = Doc2Vec(min_count=20, workers=14,)
model.build_vocab(texts_all)
model.train([doc1, doc2], total_examples=len(texts_all), epochs=50, report_delay = 600)
model.init_sims(replace=True)
model.save("path_to_model", ignore=[])
print len(model.wv.vocab), ' words learnt in model'

doc0_inferred = model.infer_vector([u'word1', u'word2', u'word3'])
sims_to_infer = model.docvecs.most_similar([doc0_inferred], topn=len(model.docvecs) )
for pair in sims_to_infer[:10]:
          id = pair[0]
          print pair
          print ''.join(id_2_text[id].words)



#word2vec
from gensim.models import Word2Vec
from gensim.models import Phrases

text_segmented = [[word1, word2], [worda, wordb]]
bigram = Phrases(text_segmented)

model = Word2Vec(bigram[text_segmented], workers=8, size=100, min_count=10, window=5, sample=100000, sg=1)
pairs_sim = model.most_similar(target_w, topn=20)
word_vector = model.wv['computer']


# handle inf and null
df = df.loc[( df['x'].notnull() ) & ( np.isfinite(df['x']) )]
df['x'] = df['x'].replace([np.inf, -np.inf], mean_x)

     A    B   C  D
0  NaN  2.0 NaN  0
1  3.0  4.0 NaN  1
2  NaN  NaN NaN  5
>>> df.dropna(axis=1, how='all')
     A    B  D
0  NaN  2.0  0
1  3.0  4.0  1
2  NaN  NaN  5

# is null
pd.isnull(YOUR_VARIABLE)

#merge df
def join_dfs(ldf, rdf):
    cols_to_use = rdf.columns.difference(ldf.columns)
    res = pd.merge(ldf, rdf[cols_to_use], left_index=True, right_index=True, how='inner')
    return res
df_merged = reduce(join_dfs, dfs)

res = pd.merge(df1, df2, left_on=column_left, right_on=column_right, how='inner')

# Return a new array of given shape and type, filled with zeros.
np.zeros((2, 1))
# array([[ 0.],
       [ 0.]])

#set value in dataframe
df.set_value(index, 'row_name', some_value)

#### drop duplicate
df1 = df1.drop_duplicates(subset=['COL1_DROPPED', 'COL2_DROPPED'], keep='first', inplace=False)

#### rename
df1 = df1.rename(columns={'old_col1':'new_col1', 'old_col2':'new_col2'})

#### sort
df1 = df1.sort_values(by=['col1', 'col2', 'col3'])

#### convert 1 row to a dict
dict_row = df1.iloc[INDEX].to_dict()
